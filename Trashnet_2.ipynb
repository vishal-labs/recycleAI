{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "whN5JIEiLmVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'dataset-resized/paper/paper502.jpg'.split('/')[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n9UeBwrNLuM_",
        "outputId": "673c9148-4ec4-4d0d-fa8b-5c4620d681d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'paper502.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data\n",
        "!mkdir data\n",
        "!mkdir data/train\n",
        "!mkdir data/test\n",
        "!mkdir data/valid\n",
        "\n",
        "for folder_name in glob('drive/MyDrive/dataset-resized/*'):\n",
        "    category_name = folder_name.split('/')[-1]\n",
        "    !mkdir data/train/{category_name}\n",
        "    !mkdir data/test/{category_name}\n",
        "    !mkdir data/valid/{category_name}"
      ],
      "metadata": {
        "id": "r7Y78r0sMvMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.8\n",
        "test_size = 0.1\n",
        "validation_size = 0.1\n",
        "\n",
        "\n",
        "def move_files(full_string, set_name):\n",
        "    image_name = full_string.split('/')[-1]\n",
        "    file_location = f\"data/{set_name}/{category_name}/{image_name}\"\n",
        "    !ln -s $(pwd)/{full_string} $(pwd)/{file_location}\n",
        "\n",
        "\n",
        "for folder_name in glob('drive/MyDrive/dataset-resized/*'):\n",
        "    file_names = glob(f\"{folder_name}/*\")\n",
        "    train, proto_test = train_test_split(file_names, train_size=0.8)\n",
        "    test, valid = train_test_split(proto_test, train_size=0.5)\n",
        "    category_name = folder_name.split('/')[-1]\n",
        "    print(f\"{category_name} train: {len(train)}\")\n",
        "    print(f\"{category_name} test: {len(test)}\")\n",
        "    for tr_filename_full_string in train:\n",
        "        move_files(tr_filename_full_string, 'train')\n",
        "    for tr_filename_full_string in valid:\n",
        "        move_files(tr_filename_full_string, 'valid')\n",
        "    for tr_filename_full_string in test:\n",
        "        move_files(tr_filename_full_string, 'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQcNXYhfN2UK",
        "outputId": "f29c8ae2-8945-456d-ec7f-8bb3d2a55a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plastic train: 385\n",
            "plastic test: 48\n",
            "glass train: 400\n",
            "glass test: 50\n",
            "metal train: 328\n",
            "metal test: 41\n",
            "cardboard train: 322\n",
            "cardboard test: 40\n",
            "trash train: 109\n",
            "trash test: 14\n",
            "paper train: 475\n",
            "paper test: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "hsKjEWo0pqJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab: install CLIP\n",
        "!pip -q install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import os, torch, numpy as np\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)  # CLIP image encoder\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF-PEgqKTDMt",
        "outputId": "814c6a7e-b9f5-45d7-f0a9-9fac206f972e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 54.4MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIP(\n",
              "  (visual): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (resblocks): Sequential(\n",
              "      (0): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (1): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (2): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (3): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (4): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (5): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (6): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (7): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (8): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (9): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (10): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (11): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (token_embedding): Embedding(49408, 512)\n",
              "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_root = \"/content/data/train\"   # update path if needed\n",
        "test_root  = \"/content/data/test\"\n",
        "\n",
        "train_ds = ImageFolder(train_root, transform=preprocess)\n",
        "test_ds  = ImageFolder(test_root,  transform=preprocess)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "num_classes = len(class_names)\n",
        "print(class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7La1qwbUTDKb",
        "outputId": "633a26e7-9b2f-449a-db61-005baefbf707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def embed_split(data_loader):\n",
        "    all_feats, all_labels = [], []\n",
        "    for imgs, labels in data_loader:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        feats = model.encode_image(imgs)                 # [B, D]\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True) # normalize (recommended)\n",
        "        all_feats.append(feats.cpu())\n",
        "        all_labels.append(labels)\n",
        "    X = torch.cat(all_feats).float()\n",
        "    y = torch.cat(all_labels).long()\n",
        "    return X, y\n",
        "\n",
        "X_tr, y_tr = embed_split(train_loader)\n",
        "X_te, y_te = embed_split(test_loader)\n",
        "\n",
        "torch.save({\"X\": X_tr, \"y\": y_tr, \"classes\": class_names}, \"train_emb.pt\")\n",
        "torch.save({\"X\": X_te, \"y\": y_te, \"classes\": class_names}, \"test_emb.pt\")\n",
        "\n",
        "in_dim = X_tr.shape[1]  # embedding dimension (e.g., 512 for ViT-B/32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC6tySVcTDE6",
        "outputId": "a0b34803-a384-4bc9-c827-7aeedf9005e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple MLP head\n",
        "mlp = nn.Sequential(\n",
        "    nn.Linear(in_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(256, num_classes)\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(mlp.parameters(), lr=2e-3, weight_decay=1e-2)\n",
        "\n",
        "def to_loader(X, y, batch_size=256, shuffle=True):\n",
        "    ds = torch.utils.data.TensorDataset(X, y)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "train_emb_loader = to_loader(X_tr, y_tr)\n",
        "test_emb_loader  = to_loader(X_te, y_te, shuffle=False)\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(20):\n",
        "    mlp.train()\n",
        "    for xb, yb in train_emb_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(mlp(xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Eval\n",
        "    mlp.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_emb_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = mlp(xb).argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.numel()\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch {epoch+1}: test acc={acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhYL-h0hTDCm",
        "outputId": "97d08e01-9b94-491f-b2e6-c6fd7f15ccc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: test acc=0.6349\n",
            "Epoch 2: test acc=0.7103\n",
            "Epoch 3: test acc=0.7937\n",
            "Epoch 4: test acc=0.8492\n",
            "Epoch 5: test acc=0.8532\n",
            "Epoch 6: test acc=0.8651\n",
            "Epoch 7: test acc=0.8690\n",
            "Epoch 8: test acc=0.8849\n",
            "Epoch 9: test acc=0.9008\n",
            "Epoch 10: test acc=0.9087\n",
            "Epoch 11: test acc=0.9048\n",
            "Epoch 12: test acc=0.9206\n",
            "Epoch 13: test acc=0.9127\n",
            "Epoch 14: test acc=0.9206\n",
            "Epoch 15: test acc=0.9167\n",
            "Epoch 16: test acc=0.9206\n",
            "Epoch 17: test acc=0.9246\n",
            "Epoch 18: test acc=0.9206\n",
            "Epoch 19: test acc=0.9246\n",
            "Epoch 20: test acc=0.9206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_image(path):\n",
        "    img = preprocess(Image.open(path)).unsqueeze(0).to(device)\n",
        "    feat = model.encode_image(img)\n",
        "    feat = feat / feat.norm(dim=-1, keepdim=True)\n",
        "    logits = mlp(feat)\n",
        "    idx = logits.argmax(dim=1).item()\n",
        "    return idx, class_names[idx]\n",
        "\n",
        "# Example:\n",
        "idx, name = predict_image(\"/content/SCR-20251027-osvn.jpeg\")\n",
        "print(idx, name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcBbFg4yTC_d",
        "outputId": "2d5106c0-cacc-4c00-e46e-ffdf004b4af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 cardboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training your MLP head on CLIP embeddings\n",
        "ckpt = {\n",
        "    \"mlp_state\": mlp.state_dict(),\n",
        "    \"classes\": class_names,\n",
        "    \"clip_name\": \"ViT-B/32\",\n",
        "    \"in_dim\": X_tr.shape[1],      # e.g., 512\n",
        "    \"mlp_hidden\": 256\n",
        "}\n",
        "torch.save(ckpt, \"recycler_mlp.pth\")  # common .pth/.pt extension [web:22]\n"
      ],
      "metadata": {
        "id": "6bTdkm7eTC22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8NUqoOCMTCs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Encode an image:\n",
        "img_emb = model.encode(Image.open('SCR-20251019-ixfs.jpeg'))\n",
        "\n",
        "text_emb = model.encode(['Coke trash can of red color', 'paper trash of any shape and size', 'Glass trash of any size and surface'])\n",
        "\n",
        "#Compute cosine similarities\n",
        "cos_scores = util.cos_sim(img_emb, text_emb)\n",
        "print(cos_scores)\n",
        "\n",
        "print(img_emb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4BwPRniphmQ",
        "outputId": "1a2c1e88-a99b-46db-d21c-6a0274d831c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3216, 0.2486, 0.2348]])\n",
            "(512,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/dataset-resized/\""
      ],
      "metadata": {
        "id": "CJePKFdsr8a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the different fields for the Model to predict,\n",
        "- cardboard  \n",
        "- glass  \n",
        "- metal  \n",
        "- paper\n",
        "- plastic  \n",
        "- trash\n",
        "1. Let us take the embeddings of each section and then, pass that to a DNN\n",
        "2. We can use a softmax function at the end layer to predict which of the 5 categories is the input image."
      ],
      "metadata": {
        "id": "Q7D4vQ2Tsbjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch: dataset, model, training loop\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Paths: main/cardboard, main/glass, main/metal, main/paper, main/plastic, main/trash\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "train_ds = datasets.ImageFolder(root=\"data/train\", transform=train_tfms)  # folders define targets\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "# Determine flattened input dimension (H*W*C = 224*224*3 here)\n",
        "in_dim = 224*224*3\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(in_dim, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(512, 6)  # logits for 6 classes\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()        # expects integer class indices (0..5)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    for x, y in train_loader:            # y are class indices from folder names\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Inference: probs = logits.softmax(dim=1)\n"
      ],
      "metadata": {
        "id": "rOfwXgWEsT_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('embeddings_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "X4DqMXwI6s1A",
        "outputId": "245d425d-9619-4da1-b049-e51a5817d397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Sequential' object has no attribute 'save'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2605107016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embeddings_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1962\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1963\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'save'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f2r0VPX96t1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}